{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lkube\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lkube\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Authorization:  <tweepy.api.API object at 0x00000270C3459748>\n",
      "Number of result tweets:  4000\n",
      "Saved 4000 documents to DB collection books_project book_tweets\n"
     ]
    }
   ],
   "source": [
    "# Basic code below is from the Get-Tweets.py and from the Async8.2 coding\n",
    "\n",
    "# Important libraries; and login information\n",
    "import tweepy\n",
    "import json\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from twitter_login_fn import oauth_login\n",
    "from twitter_login_fn import appauth_login\n",
    "from DB_fn import save_to_DB\n",
    "from DB_fn import load_from_DB\n",
    "from operator import itemgetter\n",
    "from DB_fn import load_from_DB\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define arguments for use later in the program\n",
    "arg1 = '#books' # Looking for tweets with a specific hashtag\n",
    "arg2 = 4000 # Looking for 4000 tweets\n",
    "arg3 = 'books_project' # Naming the database\n",
    "arg4 = 'book_tweets' # Naming the collection\n",
    "\n",
    "# Setting a search function \n",
    "def twitter_search(api, query, max_results=20):\n",
    "    search_results = [status for status in tweepy.Cursor(api.search, q=query).items(max_results)]\n",
    "    tweetsvoice = [tweet._json for tweet in search_results]\n",
    "    return tweetsvoice\n",
    "\n",
    "# Setting up command line arguments\n",
    "if __name__ == '__main__':\n",
    "    args = sys.argv[1:]\n",
    "    query = arg1\n",
    "    num_tweets = int(arg2)\n",
    "    DBname = arg3\n",
    "    DBcollection = arg4\n",
    "\n",
    "# Oauth Login\n",
    "    api = appauth_login()\n",
    "    print (\"Twitter Authorization: \", api)\n",
    "    \n",
    "# Accessing tweet results\n",
    "    result_tweets = twitter_search(api, query, max_results=num_tweets)\n",
    "    print ('Number of result tweets: ', len(result_tweets))\n",
    "\n",
    "# Saving results into a database\n",
    "    DBname = DBname.lower()\n",
    "    DBname = DBname.replace('#', '')\n",
    "    DBname = DBname.replace(' ', '')\n",
    "    DBcollection = DBcollection.lower()\n",
    "    DBcollection = DBcollection.replace('#', '')\n",
    "    DBcollection = DBcollection.replace(' ', '')\n",
    "    \n",
    "# Saving/loading database\n",
    "    save_to_DB(DBname, DBcollection, result_tweets)\n",
    "    load_from_DB(DBname, DBcollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TwitterStream',\n",
       " 'admin',\n",
       " 'bball',\n",
       " 'books_0530',\n",
       " 'books_project',\n",
       " 'config',\n",
       " 'disney',\n",
       " 'local',\n",
       " 'mylib',\n",
       " 'people_DB',\n",
       " 'peopledb',\n",
       " 'ppl_database',\n",
       " 'team_blake',\n",
       " 'team_kelly',\n",
       " 'team_legend',\n",
       " 'team_nick',\n",
       " 'the_voice',\n",
       " 'usgs']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing library and setting up to view data in MongoDB\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "client.list_database_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book_tweets']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing database; and available collection\n",
    "db = client.books_project\n",
    "db.list_collection_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming collection\n",
    "coll = db.book_tweets\n",
    "docs = coll.find()\n",
    "\n",
    "# Converting the document cursor to a list of documents\n",
    "doclist = list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('5edd8249dc2ca9b657c8fdf4'), 'created_at': 'Mon Jun 08 00:08:50 +0000 2020', 'id': 1269783407233576960, 'id_str': '1269783407233576960', 'text': \"RT @InderjitkaurALS: Hi #WritingCommunity, say #hello to all our lovely #writers. Let's do a #writerslift. Leave your links, and promise to…\", 'truncated': False, 'entities': {'hashtags': [{'text': 'WritingCommunity', 'indices': [24, 41]}, {'text': 'hello', 'indices': [47, 53]}, {'text': 'writers', 'indices': [72, 80]}, {'text': 'writerslift', 'indices': [93, 105]}], 'symbols': [], 'user_mentions': [{'screen_name': 'InderjitkaurALS', 'name': 'Inderjit Kaur', 'id': 1904467544, 'id_str': '1904467544', 'indices': [3, 19]}], 'urls': []}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 824725226114220033, 'id_str': '824725226114220033', 'name': 'Jellybeans Closet 4U', 'screen_name': 'JellybeansClos1', 'location': 'Pennsylvania, USA', 'description': \"I sell new and gently used women's clothes and accessories on Ebay. https://t.co/WJqjHUtBdQ\", 'url': None, 'entities': {'description': {'urls': [{'url': 'https://t.co/WJqjHUtBdQ', 'expanded_url': 'http://stores.ebay.com/jellybeanscloset4u', 'display_url': 'stores.ebay.com/jellybeansclos…', 'indices': [68, 91]}]}}, 'protected': False, 'followers_count': 7940, 'friends_count': 8534, 'listed_count': 57, 'created_at': 'Thu Jan 26 21:06:31 +0000 2017', 'favourites_count': 3291, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 109273, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/824950172123852801/Luhq0LAB_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/824950172123852801/Luhq0LAB_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'retweeted_status': {'created_at': 'Sun Jun 07 22:26:36 +0000 2020', 'id': 1269757680500916224, 'id_str': '1269757680500916224', 'text': \"Hi #WritingCommunity, say #hello to all our lovely #writers. Let's do a #writerslift. Leave your links, and promise… https://t.co/83qoWIwOgB\", 'truncated': True, 'entities': {'hashtags': [{'text': 'WritingCommunity', 'indices': [3, 20]}, {'text': 'hello', 'indices': [26, 32]}, {'text': 'writers', 'indices': [51, 59]}, {'text': 'writerslift', 'indices': [72, 84]}], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'https://t.co/83qoWIwOgB', 'expanded_url': 'https://twitter.com/i/web/status/1269757680500916224', 'display_url': 'twitter.com/i/web/status/1…', 'indices': [117, 140]}]}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 1904467544, 'id_str': '1904467544', 'name': 'Inderjit Kaur', 'screen_name': 'InderjitkaurALS', 'location': 'Mumbai ', 'description': '#Writer-Penguin India\\n& Notionpress\\n\\n\\n\\n#Podcaster #Blogger #Alivingseries\\n#Motivator #HumanRights  \\n#Socialmedia promotions #BookCovers \\n\\n\\nSpreadKindness 🖋', 'url': 'https://t.co/BY8EO4BSKU', 'entities': {'url': {'urls': [{'url': 'https://t.co/BY8EO4BSKU', 'expanded_url': 'https://linktr.ee/InderjitKaur', 'display_url': 'linktr.ee/InderjitKaur', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 12371, 'friends_count': 10665, 'listed_count': 329, 'created_at': 'Wed Sep 25 14:43:24 +0000 2013', 'favourites_count': 56221, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 32521, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1245308418983743490/SwWisXTn_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1245308418983743490/SwWisXTn_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1904467544/1566467906', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'regular'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 8, 'favorite_count': 7, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}, 'is_quote_status': False, 'retweet_count': 8, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'en'}]\n"
     ]
    }
   ],
   "source": [
    "# This is how the tweets look now\n",
    "print(doclist[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msglist = [doc['text'] for doc in doclist if 'text' in doc.keys()]\n",
    "len(msglist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = [tok for text in msglist for tok in nltk.word_tokenize(text)]\n",
    "len(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@',\n",
       " 'InderjitkaurALS',\n",
       " ':',\n",
       " 'Hi',\n",
       " '#',\n",
       " 'WritingCommunity',\n",
       " ',',\n",
       " 'say',\n",
       " '#',\n",
       " 'hello',\n",
       " 'to',\n",
       " 'all',\n",
       " 'our',\n",
       " 'lovely',\n",
       " '#',\n",
       " 'writers',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'do',\n",
       " 'a',\n",
       " '#',\n",
       " 'writerslift',\n",
       " '.',\n",
       " 'Leave',\n",
       " 'your',\n",
       " 'links',\n",
       " ',',\n",
       " 'and',\n",
       " 'promise',\n",
       " 'to…',\n",
       " 'RT',\n",
       " '@',\n",
       " 'noveliciouss',\n",
       " ':',\n",
       " 'We',\n",
       " 'live',\n",
       " 'for',\n",
       " '#',\n",
       " 'books',\n",
       " '.',\n",
       " '—',\n",
       " 'Umberto',\n",
       " 'Eco',\n",
       " '#',\n",
       " 'writing',\n",
       " '#',\n",
       " 'amwriting',\n",
       " '#']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 12180),\n",
       " (':', 6015),\n",
       " ('@', 3653),\n",
       " ('RT', 2645),\n",
       " ('https', 2643),\n",
       " (',', 2041),\n",
       " ('.', 1872),\n",
       " ('the', 1247),\n",
       " ('!', 1155),\n",
       " ('of', 1120),\n",
       " ('books', 1103),\n",
       " ('to', 1087),\n",
       " ('a', 1043),\n",
       " ('and', 972),\n",
       " ('for', 735),\n",
       " ('I', 689),\n",
       " ('The', 642),\n",
       " ('in', 613),\n",
       " ('on', 508),\n",
       " ('Books', 499),\n",
       " (';', 467),\n",
       " ('book', 461),\n",
       " ('is', 457),\n",
       " ('?', 450),\n",
       " ('you', 449),\n",
       " ('by', 416),\n",
       " ('...', 399),\n",
       " ('&', 368),\n",
       " ('’', 361),\n",
       " (\"'s\", 359)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFD = nltk.FreqDist(all_tokens)\n",
    "textFD.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " '@',\n",
       " 'inderjitkaurals',\n",
       " ':',\n",
       " 'hi',\n",
       " '#',\n",
       " 'writingcommunity',\n",
       " ',',\n",
       " 'say',\n",
       " '#',\n",
       " 'hello',\n",
       " 'to',\n",
       " 'all',\n",
       " 'our',\n",
       " 'lovely',\n",
       " '#',\n",
       " 'writers',\n",
       " '.',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'do',\n",
       " 'a',\n",
       " '#',\n",
       " 'writerslift',\n",
       " '.',\n",
       " 'leave',\n",
       " 'your',\n",
       " 'links',\n",
       " ',',\n",
       " 'and']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = [tok.lower() for text in msglist for\n",
    "tok in nltk.word_tokenize(text)]\n",
    "\n",
    "all_tokens[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(nltk_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def alpha_filter(w):\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'inderjitkaurals',\n",
       " 'hi',\n",
       " 'writingcommunity',\n",
       " 'say',\n",
       " 'hello',\n",
       " 'to',\n",
       " 'all',\n",
       " 'our',\n",
       " 'lovely',\n",
       " 'writers',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'do',\n",
       " 'a',\n",
       " 'writerslift',\n",
       " 'leave',\n",
       " 'your',\n",
       " 'links',\n",
       " 'and',\n",
       " 'promise',\n",
       " 'to…',\n",
       " 'rt',\n",
       " 'noveliciouss',\n",
       " 'we',\n",
       " 'live',\n",
       " 'for',\n",
       " 'books',\n",
       " 'umberto',\n",
       " 'eco']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [tok for tok in all_tokens if not alpha_filter(tok)]\n",
    "token_list[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets that have less than 50  retweets: 3445\n",
      "Number of tweets that have more than 100 retweets: 311\n"
     ]
    }
   ],
   "source": [
    "# Looking at retweet counts; displaying the information and comparing that information\n",
    "def FilterByGreaterThanRetweet(tweets, nNumberOfRetweets ):\n",
    "    lTweets = []\n",
    "    for tweet in tweets:\n",
    "        if tweet['retweet_count'] > nNumberOfRetweets:\n",
    "            lTweets.append(tweet)\n",
    "    return lTweets\n",
    "def FilterByLessThanRetweet(tweets, nNumberOfRetweets ):\n",
    "    lTweets = []\n",
    "    for tweet in tweets:\n",
    "        if tweet['retweet_count'] < nNumberOfRetweets:\n",
    "            lTweets.append(tweet)\n",
    "    return lTweets\n",
    "lTweetsLessThanTen        = FilterByLessThanRetweet(doclist, 50)\n",
    "lTweetsGreaterThanHundred = FilterByGreaterThanRetweet(doclist, 100)\n",
    "\n",
    "print(\"Number of tweets that have less than 50  retweets:\",len(lTweetsLessThanTen))\n",
    "print(\"Number of tweets that have more than 100 retweets:\",len(lTweetsGreaterThanHundred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date: Mon Jun 08 00:08:50 +0000 2020\n",
      "From: Jellybeans Closet 4U\n",
      "Message RT @InderjitkaurALS: Hi #WritingCommunity, say #hello to all our lovely #writers. Let's do a #writerslift. Leave your links, and promise to…\n",
      "Number of Retweets: 8\n",
      "\n",
      "Date: Mon Jun 08 00:08:49 +0000 2020\n",
      "From: Edgar Alvarado13 e.v.🌞\n",
      "Message RT @noveliciouss: We live for #books.\n",
      "— Umberto Eco\n",
      "#writing #amwriting #reading #bookslover #love #books\n",
      "#Art Zirine https://t.co/ng2kkWey…\n",
      "Number of Retweets: 19\n",
      "\n",
      "Date: Mon Jun 08 00:08:46 +0000 2020\n",
      "From: CircleOfBooks\n",
      "Message RT @CircleofBooks: #scifi #kindle #iartg #asmsg #books\n",
      "Amidst Alien Stars\n",
      "https://t.co/QW9ESAhGBf \n",
      "@CGrahamSciFi \n",
      "#tbr #asmsg #iartg #amrea…\n",
      "Number of Retweets: 9\n",
      "\n",
      "Date: Mon Jun 08 00:08:44 +0000 2020\n",
      "From: Literazee\n",
      "Message Reading The Catcher in the Rye in a social way https://t.co/GDEQ3CCx8c Not just #books, #comics and #graphicnovels… https://t.co/nAhb4GJKDn\n",
      "Number of Retweets: 0\n",
      "\n",
      "Date: Mon Jun 08 00:07:48 +0000 2020\n",
      "From: WHBrown\n",
      "Message #writing #amwriting #reading #bookslover #love #books https://t.co/A0sJauBT5u\n",
      "Number of Retweets: 0\n",
      "\n",
      "Date: Mon Jun 08 00:07:44 +0000 2020\n",
      "From: Greylupine710\n",
      "Message Someone's firing off a gun at the mall. I thought I heard shots, but I wasn't sure. #writerslife #amwriting… https://t.co/HBWYZevtwH\n",
      "Number of Retweets: 0\n",
      "\n",
      "Date: Mon Jun 08 00:07:34 +0000 2020\n",
      "From: JonnyEastCoast\n",
      "Message RT @rtArtBoost: 💥#ARTSHARE💥\n",
      "\n",
      "⭐️Introduce yourself\n",
      "⭐️Post your art/links/books/music below (SFW ONLY)\n",
      "⭐️Explore &amp; Connect with others!\n",
      "⭐RT/L…\n",
      "Number of Retweets: 2\n",
      "\n",
      "Date: Mon Jun 08 00:07:16 +0000 2020\n",
      "From: PamE\n",
      "Message RT @FilmPrimrose: Which example of #TrueLove torn apart broke your heart more!? #MarianneDashwood +#JohnWilloughby = #SenseandSensibility o…\n",
      "Number of Retweets: 4\n",
      "\n",
      "Date: Mon Jun 08 00:07:14 +0000 2020\n",
      "From: Andrew Lennon\n",
      "Message Review of THE HOUSEMATES by Iain Rob Wright \n",
      "\n",
      "#bookstagram #horrorbooks #kindle #reading #books #iainrobwright… https://t.co/mEbqdU3ozt\n",
      "Number of Retweets: 0\n",
      "\n",
      "Date: Mon Jun 08 00:07:04 +0000 2020\n",
      "From: Ophelia Kee Paranormal Romance\n",
      "Message #books #indieauthor #amwriting #bookpromotion https://t.co/bgyalaNNoh\n",
      "FREE! Today and Tomorrow only. The carriage t… https://t.co/X8kQuXLMdD\n",
      "Number of Retweets: 0\n"
     ]
    }
   ],
   "source": [
    "# Display some of the tweets with info about date, user, message, and number of retweets.\n",
    "def print_tweet_data(tweets):\n",
    "    for tweet in tweets:\n",
    "        print('\\nDate:', tweet['created_at'])\n",
    "        print('From:', tweet['user']['name'])\n",
    "        print('Message', tweet['text'])\n",
    "        print('Number of Retweets:',tweet['retweet_count'])\n",
    "        \n",
    "# Display some of the tweets with info about date, user, message, and number of retweets.\n",
    "print_tweet_data(lTweetsLessThanTen[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date: Mon Jun 08 00:08:11 +0000 2020\n",
      "From: Jen\n",
      "Message RT @StevenLenton: Celebrating over a million book sales (how did that happen?!) with a lovely giveaway! Simply follow, retweet and be in th…\n",
      "Number of Retweets: 1006\n",
      "\n",
      "Date: Mon Jun 08 00:04:35 +0000 2020\n",
      "From: Valery Sobolev\n",
      "Message RT @judehaste_write: \"An Adrenalin Rush\"\n",
      "#StayPositive_StaySafe \n",
      "Let the books do the walking #escapism \n",
      "🌈USA &amp; UK 🌈 https://t.co/78iawh5K3…\n",
      "Number of Retweets: 536\n",
      "\n",
      "Date: Mon Jun 08 00:04:24 +0000 2020\n",
      "From: Valery Sobolev\n",
      "Message RT @judehaste_write: #worldbookday2020 thanks to all the #authors #artists who support and share our works. https://t.co/7ewHDOHE2P  \n",
      "🔆 #Re…\n",
      "Number of Retweets: 399\n",
      "\n",
      "Date: Mon Jun 08 00:03:16 +0000 2020\n",
      "From: L. G. Cullens\n",
      "Message RT @LGCullens: #EcoFiction  #Adventure  #Nature #Books\n",
      "https://t.co/VGLD7eCrwi https://t.co/nHdg6kwHjV\n",
      "Number of Retweets: 205\n",
      "\n",
      "Date: Mon Jun 08 00:02:51 +0000 2020\n",
      "From: KzS Seebär\n",
      "Message RT @anglrsg9: 'I totally loved this book; it is refreshing REAL HORROR! \n",
      "I bow in gratitude!⭐️⭐️⭐️⭐️⭐️\n",
      "Jo–Amazon\n",
      "'Haven't enjoyed horror th…\n",
      "Number of Retweets: 107\n",
      "\n",
      "Date: Mon Jun 08 00:02:06 +0000 2020\n",
      "From: Catherine Mesick\n",
      "Message RT @anglrsg9: 'I totally loved this book; it is refreshing REAL HORROR! \n",
      "I bow in gratitude!⭐️⭐️⭐️⭐️⭐️\n",
      "Jo–Amazon\n",
      "'Haven't enjoyed horror th…\n",
      "Number of Retweets: 107\n",
      "\n",
      "Date: Mon Jun 08 00:01:09 +0000 2020\n",
      "From: Juanita J Pavez.\n",
      "Message RT @kevinansbro: \"A whirlwind of a book!\"\n",
      "\"A stroll through the darkest parts of humanity...\"\n",
      "\"A freaking beautiful thing...\"\n",
      "\n",
      "🔪🙈 #TheFishT…\n",
      "Number of Retweets: 1455\n",
      "\n",
      "Date: Sun Jun 07 23:57:55 +0000 2020\n",
      "From: KzS Seebär\n",
      "Message RT @judehaste_write: #worldbookday2020 thanks to all the #authors #artists who support and share our works. https://t.co/7ewHDOHE2P  \n",
      "🔆 #Re…\n",
      "Number of Retweets: 399\n",
      "\n",
      "Date: Sun Jun 07 23:57:47 +0000 2020\n",
      "From: KzS Seebär\n",
      "Message RT @judehaste_write: #Comedy 🇬🇧 #QuarantineLife &amp; let the #books do the walking! https://t.co/kkTWtiW12i #LondonTogether #romcom #JudeHaste…\n",
      "Number of Retweets: 377\n",
      "\n",
      "Date: Sun Jun 07 23:57:34 +0000 2020\n",
      "From: Juanita J Pavez.\n",
      "Message RT @paul_cude: Check out this great promo for some https://t.co/n8QhWXqObo #FREE #fantasy #scifi  #books #fantasyreads… #fantasybooks #fant…\n",
      "Number of Retweets: 293\n"
     ]
    }
   ],
   "source": [
    "# Display some of the tweets with info about date, user, message, and number of retweets.\n",
    "print_tweet_data(lTweetsGreaterThanHundred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: python twitter_hashtags.py <DBname> <DBcollection> <number>\n",
      "Top 20 Frequency Hashtags\n",
      "books 955\n",
      "Books 433\n",
      "DataScience 286\n",
      "BigData 280\n",
      "Analytics 260\n",
      "IoT 225\n",
      "IIoT 224\n",
      "Python 186\n",
      "Kindle 171\n",
      "IARTG 157\n",
      "PyTorch 129\n",
      "RStats 124\n",
      "Amazon 120\n",
      "reading 117\n",
      "book 109\n",
      "WritingCommunity 100\n",
      "writerslift 91\n",
      "amreading 80\n",
      "NLProc 80\n",
      "ebook 72\n"
     ]
    }
   ],
   "source": [
    "# Code from class to get hashtag, mentions, urls frequencies\n",
    "\n",
    "def get_entities(tweet):\n",
    "    if 'entities' in tweet.keys():\n",
    "        mentions = [user_mention['screen_name'] for user_mention in tweet['entities']['user_mentions']]\n",
    "        hashtags = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]    \n",
    "        urls = [urlitem['url'] for urlitem in tweet['entities']['urls']]    \n",
    "        urls = urls + [urlitem['expanded_url'] for urlitem in tweet['entities']['urls']]\n",
    "        return mentions, hashtags, urls\n",
    "    else:\n",
    "        return [], [], []\n",
    "\n",
    "# Use a main so can get command line arguments\n",
    "if __name__ == '__main__':\n",
    "    args = sys.argv[1:]\n",
    "    if not args or len(args) < 3:\n",
    "        print('usage: python twitter_hashtags.py <DBname> <DBcollection> <number>')\n",
    "        #sys.exit(1) #'d this so that could be run in jupyter notebook'\n",
    "    DBname = 'books_project'\n",
    "    DBcollection = 'book_tweets'\n",
    "    limit = int(20)\n",
    "\n",
    "    tweet_results = load_from_DB(DBname, DBcollection)\n",
    "\n",
    "# Create a frequency dictionary of the hashtags\n",
    "hashtag_fd = {}\n",
    "for tweet in tweet_results:\n",
    "    (mentions, hashtags, urls) = get_entities(tweet)\n",
    "    for tag in hashtags:\n",
    "        if not tag in hashtag_fd:\n",
    "            hashtag_fd[tag] = 1\n",
    "        else:\n",
    "            hashtag_fd[tag] += 1\n",
    "\n",
    "# Sorting the dictionary by frequency values\n",
    "hashtags_sorted = sorted(hashtag_fd.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# print out the top number of words with frequencies\n",
    "# go through the first 20 tweets and find the entities\n",
    "print(\"Top\", limit, \"Frequency Hashtags\")\n",
    "for (word, frequency) in hashtags_sorted[:limit]:\n",
    "        print (word, frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
